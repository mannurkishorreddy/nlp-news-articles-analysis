{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f1e123f-0fcd-493a-b93d-57b99de4e2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0137fd41-0ffa-4b2b-85af-e99324c1c273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas-profiling in /opt/conda/lib/python3.10/site-packages (3.6.6)\n",
      "Requirement already satisfied: ydata-profiling in /opt/conda/lib/python3.10/site-packages (from pandas-profiling) (4.1.2)\n",
      "Requirement already satisfied: scipy<1.10,>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling->pandas-profiling) (1.9.3)\n",
      "Collecting pandas!=1.4.0,<1.6,>1.1 (from ydata-profiling->pandas-profiling)\n",
      "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib<3.7,>=3.2 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling->pandas-profiling) (3.6.3)\n",
      "Requirement already satisfied: pydantic<1.11,>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling->pandas-profiling) (1.10.7)\n",
      "Requirement already satisfied: PyYAML<6.1,>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling->pandas-profiling) (5.4.1)\n",
      "Requirement already satisfied: jinja2<3.2,>=2.11.1 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling->pandas-profiling) (3.1.2)\n",
      "Requirement already satisfied: visions[type_image_path]==0.7.5 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling->pandas-profiling) (0.7.5)\n",
      "Requirement already satisfied: numpy<1.24,>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling->pandas-profiling) (1.23.5)\n",
      "Requirement already satisfied: htmlmin==0.1.12 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling->pandas-profiling) (0.1.12)\n",
      "Requirement already satisfied: phik<0.13,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling->pandas-profiling) (0.12.3)\n",
      "Requirement already satisfied: requests<2.29,>=2.24.0 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling->pandas-profiling) (2.28.2)\n",
      "Requirement already satisfied: tqdm<4.65,>=4.48.2 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling->pandas-profiling) (4.64.1)\n",
      "Requirement already satisfied: seaborn<0.13,>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling->pandas-profiling) (0.12.2)\n",
      "Requirement already satisfied: multimethod<1.10,>=1.4 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling->pandas-profiling) (1.9.1)\n",
      "Requirement already satisfied: statsmodels<0.14,>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling->pandas-profiling) (0.13.5)\n",
      "Requirement already satisfied: typeguard<2.14,>=2.13.2 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling->pandas-profiling) (2.13.3)\n",
      "Requirement already satisfied: imagehash==4.3.1 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling->pandas-profiling) (4.3.1)\n",
      "Requirement already satisfied: PyWavelets in /opt/conda/lib/python3.10/site-packages (from imagehash==4.3.1->ydata-profiling->pandas-profiling) (1.4.1)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from imagehash==4.3.1->ydata-profiling->pandas-profiling) (9.5.0)\n",
      "Requirement already satisfied: attrs>=19.3.0 in /opt/conda/lib/python3.10/site-packages (from visions[type_image_path]==0.7.5->ydata-profiling->pandas-profiling) (22.2.0)\n",
      "Requirement already satisfied: networkx>=2.4 in /opt/conda/lib/python3.10/site-packages (from visions[type_image_path]==0.7.5->ydata-profiling->pandas-profiling) (3.1)\n",
      "Requirement already satisfied: tangled-up-in-unicode>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from visions[type_image_path]==0.7.5->ydata-profiling->pandas-profiling) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2<3.2,>=2.11.1->ydata-profiling->pandas-profiling) (2.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas!=1.4.0,<1.6,>1.1->ydata-profiling->pandas-profiling) (2023.3)\n",
      "Requirement already satisfied: joblib>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from phik<0.13,>=0.11.1->ydata-profiling->pandas-profiling) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<1.11,>=1.8.1->ydata-profiling->pandas-profiling) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<2.29,>=2.24.0->ydata-profiling->pandas-profiling) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<2.29,>=2.24.0->ydata-profiling->pandas-profiling) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<2.29,>=2.24.0->ydata-profiling->pandas-profiling) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<2.29,>=2.24.0->ydata-profiling->pandas-profiling) (2022.12.7)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.10/site-packages (from statsmodels<0.14,>=0.13.2->ydata-profiling->pandas-profiling) (0.5.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5.2->statsmodels<0.14,>=0.13.2->ydata-profiling->pandas-profiling) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91861786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3c045da-e1e5-4bdf-a06e-fbf18e812e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news_final_project = pd.read_parquet('https://storage.googleapis.com/msca-bdp-data-open/news_final_project/news_final_project.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e83c160f-d9be-42a0-bdb3-723f87b77cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200332, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news_final_project.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08fbaf97-cfb2-4f46-8e2b-920ae99b2ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = df_news_final_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2399e6a-b1bd-46e8-8a55-fcf4fb63049e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://en.people.cn/n3/2021/0318/c90000-983012...</td>\n",
       "      <td>2021-03-18</td>\n",
       "      <td>en</td>\n",
       "      <td>Artificial intelligence improves parking effic...</td>\n",
       "      <td>\\n\\nArtificial intelligence improves parking e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://newsparliament.com/2020/02/27/children-...</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>en</td>\n",
       "      <td>Children With Autism Saw Their Learning and So...</td>\n",
       "      <td>\\nChildren With Autism Saw Their Learning and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.dataweek.co.za/12835r</td>\n",
       "      <td>2021-03-26</td>\n",
       "      <td>en</td>\n",
       "      <td>Forget ML, AI and Industry 4.0 – obsolescence ...</td>\n",
       "      <td>\\n\\nForget ML, AI and Industry 4.0 – obsolesce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.homeoffice.consumerelectronicsnet.c...</td>\n",
       "      <td>2021-03-10</td>\n",
       "      <td>en</td>\n",
       "      <td>Strategy Analytics: 71% of Smartphones Sold Gl...</td>\n",
       "      <td>\\n\\nStrategy Analytics: 71% of Smartphones Sol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.itbusinessnet.com/2020/10/olympus-t...</td>\n",
       "      <td>2020-10-20</td>\n",
       "      <td>en</td>\n",
       "      <td>Olympus to Support Endoscopic AI Diagnosis Edu...</td>\n",
       "      <td>\\n\\nOlympus to Support Endoscopic AI Diagnosis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url        date language   \n",
       "0  http://en.people.cn/n3/2021/0318/c90000-983012...  2021-03-18       en  \\\n",
       "1  http://newsparliament.com/2020/02/27/children-...  2020-02-27       en   \n",
       "2                   http://www.dataweek.co.za/12835r  2021-03-26       en   \n",
       "3  http://www.homeoffice.consumerelectronicsnet.c...  2021-03-10       en   \n",
       "4  http://www.itbusinessnet.com/2020/10/olympus-t...  2020-10-20       en   \n",
       "\n",
       "                                               title   \n",
       "0  Artificial intelligence improves parking effic...  \\\n",
       "1  Children With Autism Saw Their Learning and So...   \n",
       "2  Forget ML, AI and Industry 4.0 – obsolescence ...   \n",
       "3  Strategy Analytics: 71% of Smartphones Sold Gl...   \n",
       "4  Olympus to Support Endoscopic AI Diagnosis Edu...   \n",
       "\n",
       "                                                text  \n",
       "0  \\n\\nArtificial intelligence improves parking e...  \n",
       "1  \\nChildren With Autism Saw Their Learning and ...  \n",
       "2  \\n\\nForget ML, AI and Industry 4.0 – obsolesce...  \n",
       "3  \\n\\nStrategy Analytics: 71% of Smartphones Sol...  \n",
       "4  \\n\\nOlympus to Support Endoscopic AI Diagnosis...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7019105-1513-49d5-8b8f-408add9cbfd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200332, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = df_news\n",
    "#news = news.sample(10000,random_state=42)\n",
    "news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04b4cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#news = news.sample(500,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87f3b8a5-8530-4157-bd93-e944ce45cc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.56 ms, sys: 0 ns, total: 2.56 ms\n",
      "Wall time: 1.66 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "#!pip install nltk\n",
    "#!pip install spacy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "677849d5-966d-40a5-bb68-e7109a8d2aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "CPU times: user 45.6 s, sys: 14.5 s, total: 1min\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "#!pip install pandarallel\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "# Assuming you have already installed the pandarallel library\n",
    "\n",
    "# Initialize pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "# Convert 'text' column to string\n",
    "news['clean_text'] = news['text'].astype(str)\n",
    "\n",
    "# Remove sentences with more than 80 words\n",
    "news['clean_text'] = news['clean_text'].parallel_apply(\n",
    "    lambda x: ' '.join([sentence for sentence in nltk.sent_tokenize(x) if len(sentence.split()) <= 80])\n",
    ")\n",
    "\n",
    "# Remove words with more than 20 characters\n",
    "news['clean_text'] = news['clean_text'].parallel_apply(\n",
    "    lambda x: ' '.join([word for word in x.split() if len(word) <= 20])\n",
    ")\n",
    "\n",
    "# Remove digits\n",
    "news['clean_text'] = news['clean_text'].apply(lambda x: re.sub(r'[0-9]', '', x))\n",
    "\n",
    "# Remove uppercase letters in parentheses\n",
    "news['clean_text'] = news['clean_text'].apply(lambda x: re.sub(r'\\([A-Z]\\)', '', x))\n",
    "\n",
    "# Remove non-word and non-space characters\n",
    "news['clean_text'] = news['clean_text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "# Remove newline characters\n",
    "news['clean_text'] = news['clean_text'].str.replace('\\n', '')\n",
    "\n",
    "# Optionally convert text to lowercase\n",
    "# news['clean_text'] = news['clean_text'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b6ed7e6-13d7-49ef-8831-d6a5983ed68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "CPU times: user 1.1 ms, sys: 127 µs, total: 1.23 ms\n",
      "Wall time: 918 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd6092af-d9b1-4f6e-880a-660134c6923e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Some urban areas of the city started to use ETC system for roadside parking spaces since July   Peoples Daily OnlineLi Wenming Thanks to the application of an artificial intelligence AIempowered roadside electronic toll collection ETC system Chinas capital city Beijing has seen significant improvement in the efficiency of parking fee collection turnover of roadside parking spots order in roadside parking as well as traffic congestion As the city further deepens its roadside parking reform the ETC system has almost covered all the roadside parking spaces in the city with the proportion of vehicles parked on roads using the system exceeding  percent With the AIempowered system drivers can park their vehicles at the parking spots on the roadside and then pay the parking charge via their mobile phones after they drive away This road used to be full of cars and even the normal lanes were occupied You could hardly move a bit during the morning and evening commute time recalled a citizen ...\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)\n",
    "news['clean_text'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "180e9c3f-7afc-435f-933a-1cd91cf51b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\n",
    "  'data science',\n",
    "  'machine learning',\n",
    "  'artificial intelligence',\n",
    "  'natural language processing',\n",
    "  'computer vision',\n",
    "  'big data',\n",
    "  'data mining',\n",
    "  'deep learning',\n",
    "  'reinforcement learning',\n",
    "  'computer vision',\n",
    "  'natural language generation',\n",
    "  'text analytics',\n",
    "  'image recognition',\n",
    "  'speech recognition',\n",
    "  'recommendation systems',\n",
    "  'fraud detection'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7318209e-dd19-48c3-8776-0fc3f06a8480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 30s, sys: 124 ms, total: 1min 30s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import re\n",
    "\n",
    "filtered_news = news[news['clean_text'].str.contains('|'.join(keywords), flags=re.IGNORECASE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5e5d405-7053-4389-b95f-de096b83e3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134051, 6)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "382458f7-39be-4ffb-b9ce-89469f7ec29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_news.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78440615-c309-45d4-baac-7172aa102f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Write the filtered_news dataframe to a pickle file\n",
    "filtered_news.to_pickle('keyword_based_filtered_articles_using_re.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "699f6aa5-bc59-4322-b2b1-453c3e4a2d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filtered_news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d3b4b-2515-4459-9c0c-2935ae52c79c",
   "metadata": {},
   "source": [
    "### NER Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31f53e5f-43d7-4f0c-b8b5-5f7753cbc400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Open the pickle file in binary mode\n",
    "with open('keyword_based_filtered_articles_using_re.pkl', 'rb') as file:\n",
    "    # Load the object from the pickle file\n",
    "    df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce623dc8-13d0-4267-b751-1d5826fc1e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 134039 entries, 0 to 200329\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   url         134039 non-null  object\n",
      " 1   date        134039 non-null  object\n",
      " 2   language    134039 non-null  object\n",
      " 3   title       134039 non-null  object\n",
      " 4   text        134039 non-null  object\n",
      " 5   clean_text  134039 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 7.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1597c635-044f-4c35-9bb6-8d1504d28c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb04335c-26e1-44fc-b7bd-d1940594523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc32c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "entities = []\n",
    "labels = []\n",
    "position_start = []\n",
    "position_end = []\n",
    "\n",
    "# Iterate over each text in the 'clean_text' column\n",
    "for text in df['clean_text']:\n",
    "    # Apply spaCy NLP pipeline to extract entities\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract entity details and append to respective lists\n",
    "    for ent in doc.ents:\n",
    "        entities.append(ent.text)\n",
    "        labels.append(ent.label_)\n",
    "        position_start.append(ent.start_char)\n",
    "        position_end.append(ent.end_char)\n",
    "        \n",
    "# Create a new DataFrame from the extracted entity details\n",
    "entity_df = pd.DataFrame({'Entities': entities, 'Labels': labels, 'Position_Start': position_start, 'Position_End': position_end})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ebcf448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_lg\n",
    "#!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cb9a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 20 rows of the entity DataFrame\n",
    "entity_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada91be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top organizations\n",
    "top_organizations = entity_df[entity_df['Labels'] == 'ORG']['Entities'].value_counts().head(10)\n",
    "\n",
    "# Extract top locations\n",
    "top_locations = entity_df[entity_df['Labels'] == 'GPE']['Entities'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209aff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top organizations\n",
    "print(\"Top Organizations:\")\n",
    "print(top_organizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7330be9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top locations\n",
    "print(\"Top Locations:\")\n",
    "print(top_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5ec668-8d39-47a0-9a23-22f8546139e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "entities = []\n",
    "labels = []\n",
    "position_start = []\n",
    "position_end = []\n",
    "\n",
    "# Iterate over each text in the 'clean_text' column\n",
    "for text in df['clean_text']:\n",
    "    # Apply spaCy NLP pipeline to extract entities\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract entity details and append to respective lists\n",
    "    for ent in doc.ents:\n",
    "        entities.append(ent.text)\n",
    "        labels.append(ent.label_)\n",
    "        position_start.append(ent.start_char)\n",
    "        position_end.append(ent.end_char)\n",
    "        \n",
    "# Create a new DataFrame from the extracted entity details\n",
    "entity_df = pd.DataFrame({'Entities': entities, 'Labels': labels, 'Position_Start': position_start, 'Position_End': position_end})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "389b1d91-138b-444b-b64b-1986fdc73c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top organizations\n",
    "top_organizations = entity_df[entity_df['Labels'] == 'ORG']['Entities'].value_counts().head(10)\n",
    "\n",
    "# Extract top locations\n",
    "top_locations = entity_df[entity_df['Labels'] == 'GPE']['Entities'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35f65931-1736-4987-b2fc-e332593ed827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Organizations:\n",
      "Entities\n",
      "AI                         372\n",
      "ChatGPT                    197\n",
      "Microsoft                  157\n",
      "Google                     127\n",
      "Gray Media Group           105\n",
      "Gray Television Inc         64\n",
      "GPT                         42\n",
      "OpenAI                      40\n",
      "Facebook                    38\n",
      "Artificial Intelligence     38\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the top organizations\n",
    "print(\"Top Organizations:\")\n",
    "print(top_organizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5eca9aae-728b-4e65-aa68-a4e194d1f55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Locations:\n",
      "Entities\n",
      "US            220\n",
      "China          79\n",
      "PRNewswire     79\n",
      "India          55\n",
      "Japan          35\n",
      "UK             30\n",
      "Russia         24\n",
      "New York       23\n",
      "Canada         21\n",
      "France         21\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the top locations\n",
    "print(\"Top Locations:\")\n",
    "print(top_locations)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
